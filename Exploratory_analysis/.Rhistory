require(tidytext)
require(tokenizers)
require(plyr)
library(dplyr)
###### top occurring words pre-stemming and no custom stop words ######
depression_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
stemmed_d_tweets <- depression_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(custom) %>%
mutate(word = wordStem(word))
library(SnowballC)
library(tm)
library(wordcloud)
library(RColorBrewer)
stemmed_d_tweets <- depression_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(custom) %>%
mutate(word = wordStem(word))
stemmed_d_tweets %>%
group_by(word) %>%
count(word, sort = TRUE)
depressed <- Corpus(VectorSource(depression_tweets$clean_text))
dtm <- TermDocumentMatrix(depressed)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
depressed <- tm_map(depressed, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(depressed)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
depressed <- tm_map(depressed, removeWords, stopwords("english"))
depressed <- tm_map(depressed, removeWords, c("can"))
dtm <- TermDocumentMatrix(depressed)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
depressed <- tm_map(depressed, removeWords, stopwords("english"))
depressed <- tm_map(depressed, removeWords, c("can"))
depressed <- tm_map(depressed, stemDocument)
dtm <- TermDocumentMatrix(depressed)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
View(d)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word[-(1:1)], freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word[-(1:1)], freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Accent"))
set.seed(1234)
wordcloud(words = d$word[-(1:1)], freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
set.seed(1234)
wordcloud(words = d$word[-(1:5)], freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
set.seed(1234)
wordcloud(words = d$word[-(1:5)], freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
# depression word cloud w/the top 5 stems
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set3"))
# depression word cloud w/the top 5 stems
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Paired"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"))
# depression word cloud w/the top 5 stems
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# depression word cloud w/the top 5 stems
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"))
fatigued <- Corpus(VectorSource(fatigue_tweets$clean_text))
fatigued <- tm_map(fatigued, removeWords, stopwords("english"))
fatigued <- tm_map(fatigued, removeWords, c("can"))
fatigued <- tm_map(fatigued, stemDocument)
Fdtm <- TermDocumentMatrix(fatigued)
Fm <- as.matrix(dtm)
Fv <- sort(rowSums(m),decreasing=TRUE)
Fd <- data.frame(word = names(v),freq=v)
head(Fd, 10)
#depression word cloud w/the top 5 stems
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"))
fatigued <- Corpus(VectorSource(fatigue_tweets$clean_text))
fatigued <- tm_map(fatigued, removeWords, stopwords("english"))
fatigued <- tm_map(fatigued, removeWords, c("can"))
fatigued <- tm_map(fatigued, stemDocument)
Fdtm <- TermDocumentMatrix(fatigued)
Fm <- as.matrix(Fdtm)
Fv <- sort(rowSums(Fm),decreasing=TRUE)
Fd <- data.frame(word = names(Fv),freq=Fv)
head(Fd, 10)
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"))
fatigued <- Corpus(VectorSource(fatigue_tweets$clean_text))
fatigued <- tm_map(fatigued, removeWords, stopwords("english"))
fatigued <- tm_map(fatigued, removeWords, c("can"))
#fatigued <- tm_map(fatigued, stemDocument)
Fdtm <- TermDocumentMatrix(fatigued)
Fm <- as.matrix(Fdtm)
Fv <- sort(rowSums(Fm),decreasing=TRUE)
Fd <- data.frame(word = names(Fv),freq=Fv)
head(Fd, 10)
fatigued <- Corpus(VectorSource(fatigue_tweets$clean_text))
fatigued <- tm_map(fatigued, removeWords, stopwords("english"))
fatigued <- tm_map(fatigued, removeWords, c("can"))
fatigued <- tm_map(fatigued, stemDocument)
Fdtm <- TermDocumentMatrix(fatigued)
Fm <- as.matrix(Fdtm)
Fv <- sort(rowSums(Fm),decreasing=TRUE)
Fd <- data.frame(word = names(Fv),freq=Fv)
head(Fd, 10)
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3.5,0.25))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(4.5,1.25))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3.5,1))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,1))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.5))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.75))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(4,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(5,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.4))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.5))
wordcloud(words = Fd$word[-(1:5)], freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
View(Fd)
View(Fm)
View(m)
View(Fdtm)
View(Fdtm)
View(Fdtm)
Fd$word <- stemCompletion(Fd$word, fatigued)
head(Fd, 10)
View(fatigued)
View(Fd)
Fd$word <- stemCompletion(Fd$word, fatigued)
head(Fd, 10)
fatigued <- Corpus(VectorSource(fatigue_tweets$clean_text))
fatigued <- tm_map(fatigued, removeWords, stopwords("english"))
fatigued <- tm_map(fatigued, removeWords, c("can"))
Fdtm <- TermDocumentMatrix(fatigued)
Fm <- as.matrix(Fdtm)
Fv <- sort(rowSums(Fm),decreasing=TRUE)
Fd <- data.frame(word = names(Fv),freq=Fv)
head(Fd, 10)
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.5))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3.5,.5))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.5))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.25))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(4,.5))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.25,
colors=brewer.pal(8, "RdBu"), scale=c(4,.75))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.45,
colors=brewer.pal(8, "RdBu"), scale=c(4,.75))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.45,
colors=brewer.pal(8, "RdBu"), scale=c(3.5,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.45,
colors=brewer.pal(8, "RdBu"), scale=c(3,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=TRUE, rot.per=0.5,
colors=brewer.pal(8, "RdBu"), scale=c(3,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=TRUE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=TRUE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(3,.75))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.55,
colors=brewer.pal(8, "RdBu"), scale=c(3,.75))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.55,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.85,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.75))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.75))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.55,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.75))
#depression word cloud w/the top 5 stems
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.55,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.75))
wordcloud(words = Fd$word[-(1:5)], freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
View(fatigue_tweets)
mean(depression_tweets$word_count, na.rm = TRUE)
mean(fatigue_tweets$word_count, na.rm = TRUE)
mean(fatigue_tweets$emoji_count, na.rm = TRUE)
mean(depression_tweets$emoji_count, na.rm = TRUE)
library(readtext)
require(stringr)
require(tidytext)
require(tokenizers)
require(plyr)
library(dplyr)
###### read in tweets datasets ######
depression_tweets <- readtext('/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/20210312_0331_all_depression_tweets.csv')
colnames(depression_tweets)[2] <- "created_at"
fatigue_tweets <- readtext('/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/20210317_0331all_covid_tweets.csv')
colnames(fatigue_tweets)[2] <- "created_at"
####### Replace common characters on Twitter with their ASCII equivalents #######
d_tweets <- depression_tweets[!duplicated(depression_tweets$tweet_text),]
f_tweets <- fatigue_tweets[!duplicated(fatigue_tweets$tweet_text),]
pattern <- c('é', '…', '—', "[‘“’”´`]", '～', '＞', '+', '&amp;')
replacement <- c('e', '...', '-', "'", '~', '＞', '+', 'and')
d_tweets <- d_tweets[, c('doc_id','username', 'tweet_text')]
d_tweets$tweet_text <- qdap::mgsub(pattern= pattern, replacement = replacement, d_tweets$tweet_text)
f_tweets <- f_tweets[, c('doc_id','username', 'tweet_text')]
f_tweets$tweet_text <- qdap::mgsub(pattern= pattern, replacement = replacement, f_tweets$tweet_text)
####### Remove html symbols #######
d_tweets$tweet_text <- str_replace_all(d_tweets$tweet_text, '&[a-z]{1,6};', '' )
f_tweets$tweet_text <- str_replace_all(f_tweets$tweet_text, '&[a-z]{1,6};', '' )
####### Remove emojis and save their counts ######
emojis <- str_extract_all(d_tweets$tweet_text,'[^[:alnum:][:punct:][:space:][\\$\\~\\=\\-\\|\\*]]+')
emojis2 <- str_extract_all(f_tweets$tweet_text,'[^[:alnum:][:punct:][:space:][\\$\\~\\=\\-\\|\\*]]+')
d_tweets$emojis <- sapply(emojis, function(x) paste(x, collapse =','))
d_tweets$emoji_count <- sapply(emojis, function(x) sum(str_length(x)))
f_tweets$emojis <- sapply(emojis2, function(x) paste(x, collapse =','))
f_tweets$emoji_count <- sapply(emojis2, function(x) sum(str_length(x)))
rm(emojis)
rm(emojis2)
d_tweets$tweet_text <- iconv(d_tweets$tweet_text, 'UTF-8', 'ASCII', '')
f_tweets$tweet_text <- iconv(f_tweets$tweet_text, 'UTF-8', 'ASCII', '')
###### Remove links ######
d_tweets$tweet_text <- str_replace_all(d_tweets$tweet_text, 'https://t.co/[a-zA-Z0-9]*', '')
f_tweets$tweet_text <- str_replace_all(f_tweets$tweet_text, 'https://t.co/[a-zA-Z0-9]*', '')
###### Remove trailing white spaces ######
d_tweets$tweet_text <- str_replace_all(d_tweets$tweet_text , '( )+', ' ')
d_tweets$tweet_text <- str_trim(d_tweets$tweet_text )
f_tweets$tweet_text <- str_replace_all(f_tweets$tweet_text , '( )+', ' ')
f_tweets$tweet_text <- str_trim(f_tweets$tweet_text )
###### Tokenization ######
words <- d_tweets %>%
unnest_tokens(word, tweet_text)
words$word <- tolower(words$word)
words <- words[!grepl('#', words$word), ]
words <- words[!grepl('@', words$word), ]
clean_d_tweets <- words %>%
group_by(doc_id, username) %>%
dplyr::summarize(clean_text = paste(word, collapse = ' '), word_count = n()) %>%
as.data.frame()
fatigue_words <- f_tweets %>%
unnest_tokens(word, tweet_text)
fatigue_words$word <- tolower(fatigue_words$word)
fatigue_words <- fatigue_words[!grepl('#', fatigue_words), ]
fatigue_words <- fatigue_words[!grepl('@', fatigue_words), ]
clean_f_tweets <- fatigue_words  %>%
group_by(doc_id, username) %>%
dplyr::summarize(clean_text = paste(word, collapse = ' '), word_count = n()) %>%
as.data.frame()
###### Join the cleaned tweets with the d_tweets dataframe
final <- join(d_tweets, clean_d_tweets, type = 'inner', by = 'doc_id')
final <- join(depression_tweets, final, type = 'inner', by = 'doc_id')
fatigue <- join(f_tweets, clean_f_tweets, type = 'inner', by = 'doc_id')
fatigue <- join(fatigue_tweets, fatigue, type = 'inner', by = 'doc_id')
###### Export to CSV ######
write.csv(final, '/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/20210312_0330_cleaned_depression_tweets.csv', row.names =  TRUE)
write.csv(fatigue, '/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/20210312_0330_cleaned_covidFatigue_tweets.csv', row.names =  TRUE)
rm(list = ls())
require(plyr)
library(dplyr)
library(SnowballC)
library(tm)
library(wordcloud)
library(RColorBrewer)
###### read in tweets datasets ######
depression_tweets <- readtext('/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/cleaned_depression_tweets.csv')
fatigue_tweets <- readtext('/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/cleaned_covidFatigue_tweets.csv')
###### top occurring words pre-stemming ######
depression_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
fatigue_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
###### Add custom stop words ######
custom <- add_row(stop_words, word = "https", lexicon = "custom")
custom <- add_row(custom, word = "http", lexicon = "custom")
custom <- add_row(custom, word = "t.co", lexicon = "custom")
#custom <- add_row(custom, word = "amp", lexicon = "custom")
###### Stemming ######
stemmed_d_tweets <- depression_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(custom) %>%
mutate(word = wordStem(word))
stemmed_f_tweets <- fatigue_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(custom) %>%
mutate(word = wordStem(word))
###### Most occurring words after stemming ######
stemmed_d_tweets %>%
group_by(word) %>%
count(word, sort = TRUE)
stemmed_f_tweets %>%
group_by(word) %>%
count(word, sort = TRUE)
###### Visualizations ######
depressed <- Corpus(VectorSource(depression_tweets$clean_text))
depressed <- tm_map(depressed, removeWords, stopwords("english"))
depressed <- tm_map(depressed, removeWords, c("can"))
depressed <- tm_map(depressed, stemDocument)
dtm <- TermDocumentMatrix(depressed)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# depression word cloud w/the top 5 stems
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"))
# depression word cloud w/o the top 5 stems
wordcloud(words = d$word[-(1:5)], freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
fatigued <- Corpus(VectorSource(fatigue_tweets$clean_text))
fatigued <- tm_map(fatigued, removeWords, stopwords("english"))
fatigued <- tm_map(fatigued, removeWords, c("can"))
Fdtm <- TermDocumentMatrix(fatigued)
Fm <- as.matrix(Fdtm)
Fv <- sort(rowSums(Fm),decreasing=TRUE)
Fd <- data.frame(word = names(Fv),freq=Fv)
head(Fd, 10)
#fatigue word cloud w/the top 5 word
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.55,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.75))
# fatigue word cloud w/o the top 5 word
wordcloud(words = Fd$word[-(1:5)], freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
###### Average words per tweet ######
mean(depression_tweets$word_count, na.rm = TRUE)
mean(fatigue_tweets$word_count, na.rm = TRUE)
###### Emojis ######
mean(depression_tweets$emoji_count, na.rm = TRUE)
mean(fatigue_tweets$emoji_count, na.rm = TRUE)
depression_tweets <- readtext('/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/cleaned_depression_tweets.csv')
fatigue_tweets <- readtext('/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/cleaned_covidFatigue_tweets.csv')
depression_tweets <- readtext('/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/cleaned_depression_tweets.csv')
fatigue_tweets <- readtext('/Users/susanchen/Documents/Depression-or-Covid-Fatigue/data/cleaned_covidFatigue_tweets.csv')
depression_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
fatigue_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
custom <- add_row(stop_words, word = "https", lexicon = "custom")
custom <- add_row(custom, word = "http", lexicon = "custom")
custom <- add_row(custom, word = "t.co", lexicon = "custom")
stemmed_d_tweets <- depression_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(custom) %>%
mutate(word = wordStem(word))
stemmed_f_tweets <- fatigue_tweets %>%
unnest_tokens(word, clean_text) %>%
anti_join(custom) %>%
mutate(word = wordStem(word))
stemmed_d_tweets %>%
group_by(word) %>%
count(word, sort = TRUE)
stemmed_f_tweets %>%
group_by(word) %>%
count(word, sort = TRUE)
depressed <- Corpus(VectorSource(depression_tweets$clean_text))
depressed <- tm_map(depressed, removeWords, stopwords("english"))
depressed <- tm_map(depressed, removeWords, c("can"))
depressed <- tm_map(depressed, stemDocument)
dtm <- TermDocumentMatrix(depressed)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"))
wordcloud(words = d$word[-(1:5)], freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
fatigued <- Corpus(VectorSource(fatigue_tweets$clean_text))
fatigued <- tm_map(fatigued, removeWords, stopwords("english"))
fatigued <- tm_map(fatigued, removeWords, c("can"))
Fdtm <- TermDocumentMatrix(fatigued)
Fm <- as.matrix(Fdtm)
Fv <- sort(rowSums(Fm),decreasing=TRUE)
Fd <- data.frame(word = names(Fv),freq=Fv)
head(Fd, 10)
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.55,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(2.5,.75))
#fatigue word cloud w/the top 5 word
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(2,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(2,.75))
wordcloud(words = Fd$word, freq = Fd$freq, min.freq = 1,
max.words=100, random.order=TRUE, rot.per=0.35,
colors=brewer.pal(8, "RdBu"), scale=c(2,.75))
wordcloud(words = Fd$word[-(1:5)], freq = Fd$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
mean(depression_tweets$word_count, na.rm = TRUE)
mean(fatigue_tweets$word_count, na.rm = TRUE)
###### Emojis ######
mean(depression_tweets$emoji_count, na.rm = TRUE)
mean(fatigue_tweets$emoji_count, na.rm = TRUE)
